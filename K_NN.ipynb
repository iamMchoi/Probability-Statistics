{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K_NN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7Ngj2bYAcShh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamMchoi/Probability-Statistics/blob/master/K_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "2m7ysEXTcSf1",
        "colab_type": "text"
      },
      "source": [
        "# K-Nearest Neighbor (KNN) Project\n",
        "---\n",
        "\n",
        "_Authors: Carleton Smith; W.P.G.Peterson_\n",
        "\n",
        "\n",
        "## Project Guide\n",
        "---\n",
        "- [Project Overview](#project-overview)  \n",
        "- [Part 1: Acquire, Explore, and Preprocess Data](#part1)\n",
        "- [Part 2: Code KNN](#part2)  \n",
        "    -[KNN in sklearn](#sklearn)\n",
        "- [Part 3: Interpret Results](#part3)\n",
        "\n",
        "\n",
        "<a id=\"project-overview\"></a>\n",
        "## Project Overview\n",
        "---\n",
        "#### EXPECTED TIME: 3 HRS  \n",
        "\n",
        "This project has 3 parts:\n",
        "\n",
        "- Part 1: Familiarize yourself with the problem and data\n",
        "- Part 2: Code a KNN Classifier from scratch, evaluate performance, and compare to Scikit-Learn's implementation\n",
        "- Part 3: Interpret results and explain findings.\n",
        "\n",
        "This will include:\n",
        "- Answering simple questions regarding the data  \n",
        "- Manipulating multiple DataFrames  \n",
        "- Coding functions to:  \n",
        "    - Calculate Euclidean distance\n",
        "    - Calculate distance between many pairs of points\n",
        "    - Implement a majority voting system\n",
        "    - Combine the above to create a custom KNN algorithm\n",
        "- Use `KNeighborsClassifier` in `sklearn`  \n",
        "\n",
        "**Motivation**: KNN is a reasonably simple algorithm that is easy to grasp and can be very effective.\n",
        "\n",
        "**Objectives**: By the end of this assignment, you will:\n",
        "- Have a firm understanding of the KNN algorithm\n",
        "- Have practiced running through the data science workflow to solve a problem\n",
        "- Will demonstate how to translate a mathematical algorithm into effective code\n",
        "- Understand common pitfalls when working with distances\n",
        "\n",
        "**Problem**: Classify the type of activity a person is performing based on measurements collected from a smartphone. The activities include:  \n",
        "- Walking\n",
        "- Walking_Upstairs\n",
        "- Walking_Downstairs\n",
        "- Sitting\n",
        "- Standing\n",
        "- Laying\n",
        "\n",
        "\n",
        "**Dataset**: [_Human Activity Recognition Using Smartphones Data Set_](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) from the UC Irvine Machine Learning Repositiory.  \n",
        "\n",
        "Dataset description as provided in the original authors:\n",
        "\n",
        "---\n",
        "```\n",
        "The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. \n",
        "\n",
        "The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain. See 'features_info.txt' for more details. \n",
        "\n",
        "For each record it is provided:\n",
        "======================================\n",
        "\n",
        "- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\n",
        "- Triaxial Angular velocity from the gyroscope. \n",
        "- A 561-feature vector with time and frequency domain variables. \n",
        "- Its activity label. \n",
        "- An identifier of the subject who carried out the experiment.\n",
        "```\n",
        "\n",
        "Please see the [Data Folder](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/) to explore the data files further.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "ErrEe1GbcSf2",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"part1\"></a>\n",
        "## Part 1: Acquire, Explore, and Preprocess Data\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "caLVcPvmcSf3",
        "colab_type": "text"
      },
      "source": [
        "### Import Libraries and Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "O9CjIOZKcSf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "P3RRvblHcSf9",
        "colab_type": "text"
      },
      "source": [
        "#### Read in the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO3MeMsOhLG8",
        "colab_type": "code",
        "outputId": "ec60e549-8f15-4088-feb8-2477ef81cea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNSIUp9ShNKm",
        "colab_type": "code",
        "outputId": "cf27192b-e619-4a0c-dff9-6a400c16e41d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "DATA_PATH = \"/content/drive/My Drive/AI in Columbia/4W data/\"\n",
        "!ls \"/content/drive/My Drive/AI in Columbia/4W data\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features.txt  X_test.txt  X_train.txt  y_test.txt  y_train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "tQBPq0RIcSf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FEATURE_NAMES = DATA_PATH+\"features.txt\"\n",
        "TRAIN_DATA = DATA_PATH+\"X_train.txt\"\n",
        "TRAIN_LABELS = DATA_PATH+\"y_train.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "F-iWojGEcSgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read feature names\n",
        "feats = pd.read_table(FEATURE_NAMES, sep='\\n', header=None)\n",
        "\n",
        "# read in training data\n",
        "har_train = pd.read_table(TRAIN_DATA, sep='\\s+', header=None)\n",
        "\n",
        "# read in training labels\n",
        "har_train_labels = pd.read_table(TRAIN_LABELS, sep='\\n', header=None, names=[\"label\"], squeeze = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "4cRVCArpcSgE",
        "colab_type": "text"
      },
      "source": [
        "### Explore the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "sSV6ygJIcSgF",
        "colab_type": "text"
      },
      "source": [
        "**Print out the first five rows of the training data (`har_train`) -- does everything look okay?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "cAOEp5HTcSgG",
        "colab_type": "code",
        "outputId": "d294f694-064a-453f-877c-45b8dbe5ecc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "har_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.288585</td>\n",
              "      <td>-0.020294</td>\n",
              "      <td>-0.132905</td>\n",
              "      <td>-0.995279</td>\n",
              "      <td>-0.983111</td>\n",
              "      <td>-0.913526</td>\n",
              "      <td>-0.995112</td>\n",
              "      <td>-0.983185</td>\n",
              "      <td>-0.923527</td>\n",
              "      <td>-0.934724</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.074323</td>\n",
              "      <td>-0.298676</td>\n",
              "      <td>-0.710304</td>\n",
              "      <td>-0.112754</td>\n",
              "      <td>0.030400</td>\n",
              "      <td>-0.464761</td>\n",
              "      <td>-0.018446</td>\n",
              "      <td>-0.841247</td>\n",
              "      <td>0.179941</td>\n",
              "      <td>-0.058627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.278419</td>\n",
              "      <td>-0.016411</td>\n",
              "      <td>-0.123520</td>\n",
              "      <td>-0.998245</td>\n",
              "      <td>-0.975300</td>\n",
              "      <td>-0.960322</td>\n",
              "      <td>-0.998807</td>\n",
              "      <td>-0.974914</td>\n",
              "      <td>-0.957686</td>\n",
              "      <td>-0.943068</td>\n",
              "      <td>...</td>\n",
              "      <td>0.158075</td>\n",
              "      <td>-0.595051</td>\n",
              "      <td>-0.861499</td>\n",
              "      <td>0.053477</td>\n",
              "      <td>-0.007435</td>\n",
              "      <td>-0.732626</td>\n",
              "      <td>0.703511</td>\n",
              "      <td>-0.844788</td>\n",
              "      <td>0.180289</td>\n",
              "      <td>-0.054317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.279653</td>\n",
              "      <td>-0.019467</td>\n",
              "      <td>-0.113462</td>\n",
              "      <td>-0.995380</td>\n",
              "      <td>-0.967187</td>\n",
              "      <td>-0.978944</td>\n",
              "      <td>-0.996520</td>\n",
              "      <td>-0.963668</td>\n",
              "      <td>-0.977469</td>\n",
              "      <td>-0.938692</td>\n",
              "      <td>...</td>\n",
              "      <td>0.414503</td>\n",
              "      <td>-0.390748</td>\n",
              "      <td>-0.760104</td>\n",
              "      <td>-0.118559</td>\n",
              "      <td>0.177899</td>\n",
              "      <td>0.100699</td>\n",
              "      <td>0.808529</td>\n",
              "      <td>-0.848933</td>\n",
              "      <td>0.180637</td>\n",
              "      <td>-0.049118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.279174</td>\n",
              "      <td>-0.026201</td>\n",
              "      <td>-0.123283</td>\n",
              "      <td>-0.996091</td>\n",
              "      <td>-0.983403</td>\n",
              "      <td>-0.990675</td>\n",
              "      <td>-0.997099</td>\n",
              "      <td>-0.982750</td>\n",
              "      <td>-0.989302</td>\n",
              "      <td>-0.938692</td>\n",
              "      <td>...</td>\n",
              "      <td>0.404573</td>\n",
              "      <td>-0.117290</td>\n",
              "      <td>-0.482845</td>\n",
              "      <td>-0.036788</td>\n",
              "      <td>-0.012892</td>\n",
              "      <td>0.640011</td>\n",
              "      <td>-0.485366</td>\n",
              "      <td>-0.848649</td>\n",
              "      <td>0.181935</td>\n",
              "      <td>-0.047663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.276629</td>\n",
              "      <td>-0.016570</td>\n",
              "      <td>-0.115362</td>\n",
              "      <td>-0.998139</td>\n",
              "      <td>-0.980817</td>\n",
              "      <td>-0.990482</td>\n",
              "      <td>-0.998321</td>\n",
              "      <td>-0.979672</td>\n",
              "      <td>-0.990441</td>\n",
              "      <td>-0.942469</td>\n",
              "      <td>...</td>\n",
              "      <td>0.087753</td>\n",
              "      <td>-0.351471</td>\n",
              "      <td>-0.699205</td>\n",
              "      <td>0.123320</td>\n",
              "      <td>0.122542</td>\n",
              "      <td>0.693578</td>\n",
              "      <td>-0.615971</td>\n",
              "      <td>-0.847865</td>\n",
              "      <td>0.185151</td>\n",
              "      <td>-0.043892</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 561 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0  0.288585 -0.020294 -0.132905 -0.995279 -0.983111 -0.913526 -0.995112   \n",
              "1  0.278419 -0.016411 -0.123520 -0.998245 -0.975300 -0.960322 -0.998807   \n",
              "2  0.279653 -0.019467 -0.113462 -0.995380 -0.967187 -0.978944 -0.996520   \n",
              "3  0.279174 -0.026201 -0.123283 -0.996091 -0.983403 -0.990675 -0.997099   \n",
              "4  0.276629 -0.016570 -0.115362 -0.998139 -0.980817 -0.990482 -0.998321   \n",
              "\n",
              "        7         8         9      ...          551       552       553  \\\n",
              "0 -0.983185 -0.923527 -0.934724    ...    -0.074323 -0.298676 -0.710304   \n",
              "1 -0.974914 -0.957686 -0.943068    ...     0.158075 -0.595051 -0.861499   \n",
              "2 -0.963668 -0.977469 -0.938692    ...     0.414503 -0.390748 -0.760104   \n",
              "3 -0.982750 -0.989302 -0.938692    ...     0.404573 -0.117290 -0.482845   \n",
              "4 -0.979672 -0.990441 -0.942469    ...     0.087753 -0.351471 -0.699205   \n",
              "\n",
              "        554       555       556       557       558       559       560  \n",
              "0 -0.112754  0.030400 -0.464761 -0.018446 -0.841247  0.179941 -0.058627  \n",
              "1  0.053477 -0.007435 -0.732626  0.703511 -0.844788  0.180289 -0.054317  \n",
              "2 -0.118559  0.177899  0.100699  0.808529 -0.848933  0.180637 -0.049118  \n",
              "3 -0.036788 -0.012892  0.640011 -0.485366 -0.848649  0.181935 -0.047663  \n",
              "4  0.123320  0.122542  0.693578 -0.615971 -0.847865  0.185151 -0.043892  \n",
              "\n",
              "[5 rows x 561 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "vMBOM_8ccSgJ",
        "colab_type": "text"
      },
      "source": [
        "#### Question 1\n",
        "How many rows and columns are in `har_train`?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "veXgg0tNcSgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "\n",
        "### Find out how many rows and columns are in har_train\n",
        "### Assign the tuple of (<rows>, <cols>) to ans1\n",
        "### For your reference you may also want to print out the nubmer of rows and columns\n",
        "\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "ans1 = (7352, 561)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 1",
          "locked": true,
          "points": "5",
          "solution": false
        },
        "id": "GUh9IwBxcSgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "u4t6QGtncSgR",
        "colab_type": "text"
      },
      "source": [
        "**Print the first 5 rows of `feats` - the DataFrame of feature names.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "8LFkl4-OcSgT",
        "colab_type": "code",
        "outputId": "a1616e8e-f15f-40f0-f0f4-e795471bef8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "feats.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1 tBodyAcc-mean()-X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2 tBodyAcc-mean()-Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3 tBodyAcc-mean()-Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4 tBodyAcc-std()-X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5 tBodyAcc-std()-Y</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     0\n",
              "0  1 tBodyAcc-mean()-X\n",
              "1  2 tBodyAcc-mean()-Y\n",
              "2  3 tBodyAcc-mean()-Z\n",
              "3   4 tBodyAcc-std()-X\n",
              "4   5 tBodyAcc-std()-Y"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "0bZ0w1a3cSgW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Change the names of columns via the `.columns` attribute**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "5xKPLsONcSgX",
        "colab_type": "code",
        "outputId": "3c92ce75-67f3-405a-cae3-377253ca5fad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "har_train.columns = feats.iloc[:,0]\n",
        "har_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1 tBodyAcc-mean()-X</th>\n",
              "      <th>2 tBodyAcc-mean()-Y</th>\n",
              "      <th>3 tBodyAcc-mean()-Z</th>\n",
              "      <th>4 tBodyAcc-std()-X</th>\n",
              "      <th>5 tBodyAcc-std()-Y</th>\n",
              "      <th>6 tBodyAcc-std()-Z</th>\n",
              "      <th>7 tBodyAcc-mad()-X</th>\n",
              "      <th>8 tBodyAcc-mad()-Y</th>\n",
              "      <th>9 tBodyAcc-mad()-Z</th>\n",
              "      <th>10 tBodyAcc-max()-X</th>\n",
              "      <th>...</th>\n",
              "      <th>552 fBodyBodyGyroJerkMag-meanFreq()</th>\n",
              "      <th>553 fBodyBodyGyroJerkMag-skewness()</th>\n",
              "      <th>554 fBodyBodyGyroJerkMag-kurtosis()</th>\n",
              "      <th>555 angle(tBodyAccMean,gravity)</th>\n",
              "      <th>556 angle(tBodyAccJerkMean),gravityMean)</th>\n",
              "      <th>557 angle(tBodyGyroMean,gravityMean)</th>\n",
              "      <th>558 angle(tBodyGyroJerkMean,gravityMean)</th>\n",
              "      <th>559 angle(X,gravityMean)</th>\n",
              "      <th>560 angle(Y,gravityMean)</th>\n",
              "      <th>561 angle(Z,gravityMean)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.288585</td>\n",
              "      <td>-0.020294</td>\n",
              "      <td>-0.132905</td>\n",
              "      <td>-0.995279</td>\n",
              "      <td>-0.983111</td>\n",
              "      <td>-0.913526</td>\n",
              "      <td>-0.995112</td>\n",
              "      <td>-0.983185</td>\n",
              "      <td>-0.923527</td>\n",
              "      <td>-0.934724</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.074323</td>\n",
              "      <td>-0.298676</td>\n",
              "      <td>-0.710304</td>\n",
              "      <td>-0.112754</td>\n",
              "      <td>0.030400</td>\n",
              "      <td>-0.464761</td>\n",
              "      <td>-0.018446</td>\n",
              "      <td>-0.841247</td>\n",
              "      <td>0.179941</td>\n",
              "      <td>-0.058627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.278419</td>\n",
              "      <td>-0.016411</td>\n",
              "      <td>-0.123520</td>\n",
              "      <td>-0.998245</td>\n",
              "      <td>-0.975300</td>\n",
              "      <td>-0.960322</td>\n",
              "      <td>-0.998807</td>\n",
              "      <td>-0.974914</td>\n",
              "      <td>-0.957686</td>\n",
              "      <td>-0.943068</td>\n",
              "      <td>...</td>\n",
              "      <td>0.158075</td>\n",
              "      <td>-0.595051</td>\n",
              "      <td>-0.861499</td>\n",
              "      <td>0.053477</td>\n",
              "      <td>-0.007435</td>\n",
              "      <td>-0.732626</td>\n",
              "      <td>0.703511</td>\n",
              "      <td>-0.844788</td>\n",
              "      <td>0.180289</td>\n",
              "      <td>-0.054317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.279653</td>\n",
              "      <td>-0.019467</td>\n",
              "      <td>-0.113462</td>\n",
              "      <td>-0.995380</td>\n",
              "      <td>-0.967187</td>\n",
              "      <td>-0.978944</td>\n",
              "      <td>-0.996520</td>\n",
              "      <td>-0.963668</td>\n",
              "      <td>-0.977469</td>\n",
              "      <td>-0.938692</td>\n",
              "      <td>...</td>\n",
              "      <td>0.414503</td>\n",
              "      <td>-0.390748</td>\n",
              "      <td>-0.760104</td>\n",
              "      <td>-0.118559</td>\n",
              "      <td>0.177899</td>\n",
              "      <td>0.100699</td>\n",
              "      <td>0.808529</td>\n",
              "      <td>-0.848933</td>\n",
              "      <td>0.180637</td>\n",
              "      <td>-0.049118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.279174</td>\n",
              "      <td>-0.026201</td>\n",
              "      <td>-0.123283</td>\n",
              "      <td>-0.996091</td>\n",
              "      <td>-0.983403</td>\n",
              "      <td>-0.990675</td>\n",
              "      <td>-0.997099</td>\n",
              "      <td>-0.982750</td>\n",
              "      <td>-0.989302</td>\n",
              "      <td>-0.938692</td>\n",
              "      <td>...</td>\n",
              "      <td>0.404573</td>\n",
              "      <td>-0.117290</td>\n",
              "      <td>-0.482845</td>\n",
              "      <td>-0.036788</td>\n",
              "      <td>-0.012892</td>\n",
              "      <td>0.640011</td>\n",
              "      <td>-0.485366</td>\n",
              "      <td>-0.848649</td>\n",
              "      <td>0.181935</td>\n",
              "      <td>-0.047663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.276629</td>\n",
              "      <td>-0.016570</td>\n",
              "      <td>-0.115362</td>\n",
              "      <td>-0.998139</td>\n",
              "      <td>-0.980817</td>\n",
              "      <td>-0.990482</td>\n",
              "      <td>-0.998321</td>\n",
              "      <td>-0.979672</td>\n",
              "      <td>-0.990441</td>\n",
              "      <td>-0.942469</td>\n",
              "      <td>...</td>\n",
              "      <td>0.087753</td>\n",
              "      <td>-0.351471</td>\n",
              "      <td>-0.699205</td>\n",
              "      <td>0.123320</td>\n",
              "      <td>0.122542</td>\n",
              "      <td>0.693578</td>\n",
              "      <td>-0.615971</td>\n",
              "      <td>-0.847865</td>\n",
              "      <td>0.185151</td>\n",
              "      <td>-0.043892</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 561 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "0  1 tBodyAcc-mean()-X  2 tBodyAcc-mean()-Y  3 tBodyAcc-mean()-Z  \\\n",
              "0             0.288585            -0.020294            -0.132905   \n",
              "1             0.278419            -0.016411            -0.123520   \n",
              "2             0.279653            -0.019467            -0.113462   \n",
              "3             0.279174            -0.026201            -0.123283   \n",
              "4             0.276629            -0.016570            -0.115362   \n",
              "\n",
              "0  4 tBodyAcc-std()-X  5 tBodyAcc-std()-Y  6 tBodyAcc-std()-Z  \\\n",
              "0           -0.995279           -0.983111           -0.913526   \n",
              "1           -0.998245           -0.975300           -0.960322   \n",
              "2           -0.995380           -0.967187           -0.978944   \n",
              "3           -0.996091           -0.983403           -0.990675   \n",
              "4           -0.998139           -0.980817           -0.990482   \n",
              "\n",
              "0  7 tBodyAcc-mad()-X  8 tBodyAcc-mad()-Y  9 tBodyAcc-mad()-Z  \\\n",
              "0           -0.995112           -0.983185           -0.923527   \n",
              "1           -0.998807           -0.974914           -0.957686   \n",
              "2           -0.996520           -0.963668           -0.977469   \n",
              "3           -0.997099           -0.982750           -0.989302   \n",
              "4           -0.998321           -0.979672           -0.990441   \n",
              "\n",
              "0  10 tBodyAcc-max()-X            ...             \\\n",
              "0            -0.934724            ...              \n",
              "1            -0.943068            ...              \n",
              "2            -0.938692            ...              \n",
              "3            -0.938692            ...              \n",
              "4            -0.942469            ...              \n",
              "\n",
              "0  552 fBodyBodyGyroJerkMag-meanFreq()  553 fBodyBodyGyroJerkMag-skewness()  \\\n",
              "0                            -0.074323                            -0.298676   \n",
              "1                             0.158075                            -0.595051   \n",
              "2                             0.414503                            -0.390748   \n",
              "3                             0.404573                            -0.117290   \n",
              "4                             0.087753                            -0.351471   \n",
              "\n",
              "0  554 fBodyBodyGyroJerkMag-kurtosis()  555 angle(tBodyAccMean,gravity)  \\\n",
              "0                            -0.710304                        -0.112754   \n",
              "1                            -0.861499                         0.053477   \n",
              "2                            -0.760104                        -0.118559   \n",
              "3                            -0.482845                        -0.036788   \n",
              "4                            -0.699205                         0.123320   \n",
              "\n",
              "0  556 angle(tBodyAccJerkMean),gravityMean)  \\\n",
              "0                                  0.030400   \n",
              "1                                 -0.007435   \n",
              "2                                  0.177899   \n",
              "3                                 -0.012892   \n",
              "4                                  0.122542   \n",
              "\n",
              "0  557 angle(tBodyGyroMean,gravityMean)  \\\n",
              "0                             -0.464761   \n",
              "1                             -0.732626   \n",
              "2                              0.100699   \n",
              "3                              0.640011   \n",
              "4                              0.693578   \n",
              "\n",
              "0  558 angle(tBodyGyroJerkMean,gravityMean)  559 angle(X,gravityMean)  \\\n",
              "0                                 -0.018446                 -0.841247   \n",
              "1                                  0.703511                 -0.844788   \n",
              "2                                  0.808529                 -0.848933   \n",
              "3                                 -0.485366                 -0.848649   \n",
              "4                                 -0.615971                 -0.847865   \n",
              "\n",
              "0  560 angle(Y,gravityMean)  561 angle(Z,gravityMean)  \n",
              "0                  0.179941                 -0.058627  \n",
              "1                  0.180289                 -0.054317  \n",
              "2                  0.180637                 -0.049118  \n",
              "3                  0.181935                 -0.047663  \n",
              "4                  0.185151                 -0.043892  \n",
              "\n",
              "[5 rows x 561 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "uCDN9C6bcSgb",
        "colab_type": "text"
      },
      "source": [
        "#### Question 2\n",
        "Are there missing values?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8mFU7-SkxGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for column in har_train.columns:                  # Select one colum among columns\n",
        "  nan_total = har_train[column].isna().sum()      # Count Nan data\n",
        "  if nan_total > 0 :\n",
        "    print(column, nan_total )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2LZAZmVcSgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### How many \"null\" values are in the `har_train` dataframe\n",
        "### Assign int answer to ans1\n",
        "\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "ans1 = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 2",
          "locked": true,
          "points": "5",
          "solution": false
        },
        "id": "M_4AFGJ3cSgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "RWM21YPVcSgj",
        "colab_type": "text"
      },
      "source": [
        "**Plot the correlation plot of the first 20 features (`har_train.iloc[:, :20])` with `seaborn`.**\n",
        "\n",
        "- Seaborn: https://seaborn.pydata.org/examples/many_pairwise_correlations.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "NB3aPNeRcSgk",
        "colab_type": "code",
        "outputId": "a78258a7-9c85-427e-a338-2b84eb5cd63f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        }
      },
      "source": [
        "# seaborn\n",
        "first_twenty = har_train.iloc[:, :20] # pull out first 20 feats\n",
        "corr = first_twenty.corr()  # compute correlation matrix\n",
        "mask = np.zeros_like(corr, dtype=np.bool)  # make mask\n",
        "mask[np.triu_indices_from(mask)] = True  # mask the upper triangle\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(11, 9))  # create a figure and a subplot\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)  # custom color map\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    mask=mask,\n",
        "    cmap=cmap,\n",
        "    center=0,\n",
        "    linewidth=0.5,\n",
        "    cbar_kws={'shrink': 0.5}\n",
        ");"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr4AAAJ3CAYAAAByYFIJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f7b7f9d6668>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "KybqxlSpcSgn",
        "colab_type": "text"
      },
      "source": [
        "#### Question 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaDUNQMecSgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### In looking at the graphic above:\n",
        "### True or False:\n",
        "### In these first 20 features, some are  highly correlated. e.g. With correlation >0.5 or < -0.5?\n",
        "### Assign boolean answer to ans1\n",
        "\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "ans1 = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 3",
          "locked": true,
          "points": "5",
          "solution": false
        },
        "id": "-G8XYtOKcSgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "Ywn9jY9qcSgw",
        "colab_type": "text"
      },
      "source": [
        "EDA, as was performed (briefly) above is used to develop an idea of potential problems with data. Particularly with modeling, looking for Null / impossible values, and correlated features are important steps to:  \n",
        "1. See if any features will not be useful in models becuase of null values.\n",
        "2. See if any model assumptions are violated by correlated features (such as in linear / logistic regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "KLEPenR3cSgy",
        "colab_type": "text"
      },
      "source": [
        "**Switching to the target variable (`har_train_labels`).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "8_9nbvVqcSg1",
        "colab_type": "text"
      },
      "source": [
        "#### Question 4\n",
        "Investigate class sizes - are they unbalanced?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9lpctkqcSg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### How many times does the majority class appear in our data?\n",
        "### How many times does the minority class appear in our data?\n",
        "### Assign int to ans_maj and ans_min\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "ans_maj = 1407\n",
        "ans_min = 986"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 4",
          "locked": true,
          "points": "5",
          "solution": false
        },
        "id": "fJrKbs3xcSg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "lcub3Sw8cSg9",
        "colab_type": "text"
      },
      "source": [
        "While the activities are not perfectly represented equally, its fairly close.  \n",
        "A large imbalance in the distribution of the target variable categories can cause machine learning algorithms to train themselves well with the majority class, and perform poorly on the minority class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "T4R0owbYcSg_",
        "colab_type": "text"
      },
      "source": [
        "**Use the `.describe()` method along with `.groupby()` method to compare the statistics within each activity.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "ztDhWtR4cSg_",
        "colab_type": "code",
        "outputId": "71b6b1ab-596b-44ff-8d09-fcbd815dcae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# concatenate the target variable\n",
        "# give target and observations conventional names\n",
        "y = har_train_labels \n",
        "X = har_train\n",
        "\n",
        "data = pd.concat([X, y], axis=1)\n",
        "data.shape "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7352, 562)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "scrolled": false,
        "id": "DEvrGuQ4cShC",
        "colab_type": "code",
        "outputId": "100f13df-2bc9-43bd-a79d-cb9c4cb6b537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "# group by the 'label' and show descriptive stats\n",
        "data.groupby('label').agg(['count', 'mean','std','min','max','median']).T.head(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">1 tBodyAcc-mean()-X</th>\n",
              "      <th>count</th>\n",
              "      <td>1226.000000</td>\n",
              "      <td>1073.000000</td>\n",
              "      <td>986.000000</td>\n",
              "      <td>1286.000000</td>\n",
              "      <td>1374.000000</td>\n",
              "      <td>1407.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.276260</td>\n",
              "      <td>0.261930</td>\n",
              "      <td>0.288169</td>\n",
              "      <td>0.273449</td>\n",
              "      <td>0.279294</td>\n",
              "      <td>0.269191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.050353</td>\n",
              "      <td>0.078029</td>\n",
              "      <td>0.095101</td>\n",
              "      <td>0.041998</td>\n",
              "      <td>0.020097</td>\n",
              "      <td>0.101541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.121465</td>\n",
              "      <td>-0.061041</td>\n",
              "      <td>-0.161088</td>\n",
              "      <td>-0.412659</td>\n",
              "      <td>0.111231</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.433256</td>\n",
              "      <td>0.480180</td>\n",
              "      <td>0.617597</td>\n",
              "      <td>0.559135</td>\n",
              "      <td>0.631510</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>median</th>\n",
              "      <td>0.274582</td>\n",
              "      <td>0.266666</td>\n",
              "      <td>0.284955</td>\n",
              "      <td>0.277306</td>\n",
              "      <td>0.277507</td>\n",
              "      <td>0.276946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">2 tBodyAcc-mean()-Y</th>\n",
              "      <th>count</th>\n",
              "      <td>1226.000000</td>\n",
              "      <td>1073.000000</td>\n",
              "      <td>986.000000</td>\n",
              "      <td>1286.000000</td>\n",
              "      <td>1374.000000</td>\n",
              "      <td>1407.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.017768</td>\n",
              "      <td>-0.026647</td>\n",
              "      <td>-0.016370</td>\n",
              "      <td>-0.012143</td>\n",
              "      <td>-0.016123</td>\n",
              "      <td>-0.018345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.020880</td>\n",
              "      <td>0.037038</td>\n",
              "      <td>0.027057</td>\n",
              "      <td>0.032421</td>\n",
              "      <td>0.017846</td>\n",
              "      <td>0.073512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-0.127407</td>\n",
              "      <td>-0.183885</td>\n",
              "      <td>-0.094826</td>\n",
              "      <td>-0.121073</td>\n",
              "      <td>-0.116007</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.071488</td>\n",
              "      <td>0.100904</td>\n",
              "      <td>0.099755</td>\n",
              "      <td>0.324130</td>\n",
              "      <td>0.212768</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>median</th>\n",
              "      <td>-0.017867</td>\n",
              "      <td>-0.023000</td>\n",
              "      <td>-0.017714</td>\n",
              "      <td>-0.016457</td>\n",
              "      <td>-0.017097</td>\n",
              "      <td>-0.017364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">3 tBodyAcc-mean()-Z</th>\n",
              "      <th>count</th>\n",
              "      <td>1226.000000</td>\n",
              "      <td>1073.000000</td>\n",
              "      <td>986.000000</td>\n",
              "      <td>1286.000000</td>\n",
              "      <td>1374.000000</td>\n",
              "      <td>1407.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.108884</td>\n",
              "      <td>-0.120424</td>\n",
              "      <td>-0.105860</td>\n",
              "      <td>-0.106581</td>\n",
              "      <td>-0.107330</td>\n",
              "      <td>-0.107169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.032436</td>\n",
              "      <td>0.060204</td>\n",
              "      <td>0.050656</td>\n",
              "      <td>0.045323</td>\n",
              "      <td>0.035680</td>\n",
              "      <td>0.089743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-0.285675</td>\n",
              "      <td>-0.403290</td>\n",
              "      <td>-0.289816</td>\n",
              "      <td>-0.560934</td>\n",
              "      <td>-0.509645</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.006195</td>\n",
              "      <td>0.142537</td>\n",
              "      <td>0.091229</td>\n",
              "      <td>0.280939</td>\n",
              "      <td>0.267377</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>median</th>\n",
              "      <td>-0.110424</td>\n",
              "      <td>-0.113635</td>\n",
              "      <td>-0.109039</td>\n",
              "      <td>-0.108125</td>\n",
              "      <td>-0.108771</td>\n",
              "      <td>-0.108104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">4 tBodyAcc-std()-X</th>\n",
              "      <th>count</th>\n",
              "      <td>1226.000000</td>\n",
              "      <td>1073.000000</td>\n",
              "      <td>986.000000</td>\n",
              "      <td>1286.000000</td>\n",
              "      <td>1374.000000</td>\n",
              "      <td>1407.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.312641</td>\n",
              "      <td>-0.221072</td>\n",
              "      <td>0.139847</td>\n",
              "      <td>-0.983450</td>\n",
              "      <td>-0.985346</td>\n",
              "      <td>-0.959475</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "label                                 1            2           3            4  \\\n",
              "1 tBodyAcc-mean()-X count   1226.000000  1073.000000  986.000000  1286.000000   \n",
              "                    mean       0.276260     0.261930    0.288169     0.273449   \n",
              "                    std        0.050353     0.078029    0.095101     0.041998   \n",
              "                    min        0.121465    -0.061041   -0.161088    -0.412659   \n",
              "                    max        0.433256     0.480180    0.617597     0.559135   \n",
              "                    median     0.274582     0.266666    0.284955     0.277306   \n",
              "2 tBodyAcc-mean()-Y count   1226.000000  1073.000000  986.000000  1286.000000   \n",
              "                    mean      -0.017768    -0.026647   -0.016370    -0.012143   \n",
              "                    std        0.020880     0.037038    0.027057     0.032421   \n",
              "                    min       -0.127407    -0.183885   -0.094826    -0.121073   \n",
              "                    max        0.071488     0.100904    0.099755     0.324130   \n",
              "                    median    -0.017867    -0.023000   -0.017714    -0.016457   \n",
              "3 tBodyAcc-mean()-Z count   1226.000000  1073.000000  986.000000  1286.000000   \n",
              "                    mean      -0.108884    -0.120424   -0.105860    -0.106581   \n",
              "                    std        0.032436     0.060204    0.050656     0.045323   \n",
              "                    min       -0.285675    -0.403290   -0.289816    -0.560934   \n",
              "                    max        0.006195     0.142537    0.091229     0.280939   \n",
              "                    median    -0.110424    -0.113635   -0.109039    -0.108125   \n",
              "4 tBodyAcc-std()-X  count   1226.000000  1073.000000  986.000000  1286.000000   \n",
              "                    mean      -0.312641    -0.221072    0.139847    -0.983450   \n",
              "\n",
              "label                                 5            6  \n",
              "1 tBodyAcc-mean()-X count   1374.000000  1407.000000  \n",
              "                    mean       0.279294     0.269191  \n",
              "                    std        0.020097     0.101541  \n",
              "                    min        0.111231    -1.000000  \n",
              "                    max        0.631510     1.000000  \n",
              "                    median     0.277507     0.276946  \n",
              "2 tBodyAcc-mean()-Y count   1374.000000  1407.000000  \n",
              "                    mean      -0.016123    -0.018345  \n",
              "                    std        0.017846     0.073512  \n",
              "                    min       -0.116007    -1.000000  \n",
              "                    max        0.212768     1.000000  \n",
              "                    median    -0.017097    -0.017364  \n",
              "3 tBodyAcc-mean()-Z count   1374.000000  1407.000000  \n",
              "                    mean      -0.107330    -0.107169  \n",
              "                    std        0.035680     0.089743  \n",
              "                    min       -0.509645    -1.000000  \n",
              "                    max        0.267377     1.000000  \n",
              "                    median    -0.108771    -0.108104  \n",
              "4 tBodyAcc-std()-X  count   1374.000000  1407.000000  \n",
              "                    mean      -0.985346    -0.959475  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "Usdb2Wk_cShF",
        "colab_type": "text"
      },
      "source": [
        "**Summary**:  \n",
        "EDA should be tailored to your specific problem - to help develop and understanding of the data for a particular purpose. This is time consuming process when the data are large with many features. It's subject matter experts can guide initial explorations.  \n",
        "\n",
        "The above are examples of just a few of the things EDA should include when starting a project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "fhsO3IJecShF",
        "colab_type": "text"
      },
      "source": [
        "With a feel for the data, now we will aside a \"test\" data-set that will allow us to evaluate out models.  \n",
        "\n",
        "`train_test_split` from `sklearn.model_selection` module provides an easy way to do this.  \n",
        "\n",
        "Setting `test_size=.3` and `random_state=24`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "5Sx9obprcShH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=24)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqKqT7wzcShJ",
        "colab_type": "text"
      },
      "source": [
        "#### Question 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGmvUrvfcShK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### `test_size = .3` means:\n",
        "\n",
        "### 'a') the final 30% of the data is held out for the test data\n",
        "### 'b') any observations with \".3\" are held out\n",
        "### 'c') a random 30% of the data are held out\n",
        "### 'd') a random 70% of the data are held out\n",
        "### assign character associated with your choice to ans1 as a string\n",
        "\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "ans1 = 'c'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 5",
          "locked": true,
          "points": "5",
          "solution": false
        },
        "id": "qYR3CMNWcShN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "GnOqikPqcShR",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"part2\"></a>\n",
        "## Part 2: Code KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "3tVwApBacShT",
        "colab_type": "text"
      },
      "source": [
        "Note: The following was adapted from example 2.1.2 in Chapter 2 of [_Machine Learning in Action_ by Peter Harrington](https://www.manning.com/books/machine-learning-in-action)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "-mT6GzSYcShU",
        "colab_type": "text"
      },
      "source": [
        "Before fitting a KNN model using built in functionality in the `sklearn` package, we will code our own version of KNN.  \n",
        "\n",
        "Given a value (with our movement-data, this \"value\" is better thought of as a vector of values) to be classified, KNN calculates the distance between that value and all other values in the training data-set. Then, the \"`k`\" nearest neighbors are polled as to their \"label\", and the given value is predicted to be of that majority value.  \n",
        "\n",
        "Thus we need to:\n",
        "\n",
        "**Create a function that accepts the following parameters:**  \n",
        "- A single data point to be classified (`input_vector`)\n",
        "- Training data (`X_train`)   \n",
        "- Labels for training data\n",
        "- Value for k (some positive integer)\n",
        "- Optional: Similarity Metric (Euclidean or Cosine)- This exercise will use [Euclidean](https://en.wikipedia.org/wiki/Euclidean_distance) for simplicity.  \n",
        "\n",
        "\n",
        "**Function Signature:**  \n",
        "`def my_knn(input_vector, X_train, y_train, k, [metric])`  \n",
        "\n",
        "**Pseudo Code:**  \n",
        "```\n",
        "for every point in our dataset:\n",
        "    calculate the distance between the current point and input_vector\n",
        "    sort the distances in increasing order\n",
        "    take k items with lowest disances to input_vector\n",
        "    find the majority class among these items\n",
        "    return the majority class label from the k closest neighbors\n",
        "```\n",
        "\n",
        "**Return:**   \n",
        "- The prediction for `input_vector`  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "_WvVILdVcShV",
        "colab_type": "text"
      },
      "source": [
        "#### Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdoLFJOGppcq",
        "colab_type": "text"
      },
      "source": [
        "** Euclidean distance**\n",
        "\\begin{aligned}d(\\mathbf {p} ,\\mathbf {q} )=d(\\mathbf {q} ,\\mathbf {p} )&={\\sqrt {(q_{1}-p_{1})^{2}+(q_{2}-p_{2})^{2}+\\cdots +(q_{n}-p_{n})^{2}}}\\\\[8pt]&={\\sqrt {\\sum _{i=1}^{n}(q_{i}-p_{i})^{2}}}.\\end{aligned}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnbUIzfAcShW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### This exercise will use Euclidean distances.  \n",
        "### Please find the Euclidean distance between the points (1,2,3,-4,6) and (10,2,32,-2,0)\n",
        "### Assign distance as number to ans1\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "p1 = (1,2,3,-4,6)\n",
        "p2 = (10,2,32,-2,0)\n",
        "\n",
        "#dist = 0\n",
        "#for a, b in zip(p1,p2):     #Make list as 2 dimentionaly\n",
        "#  dist += (a-b)**2\n",
        "\n",
        "ans1 = 31.016124838541646"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 6",
          "locked": true,
          "points": "5",
          "solution": false
        },
        "id": "KpwGwmipcShY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "lO_J_s5zcShb",
        "colab_type": "text"
      },
      "source": [
        "#### Question 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SYSBl2acShb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### Code a function called \"euclid_dist\" \n",
        "\n",
        "### ACCEPT two inputs, points, represented as tuples in the format, (a1, b1,...n1), (a2, b2, ...n2).\n",
        "### RETURN the euclidean distance\n",
        "\n",
        "### Remember: \"**\" is the python operator for exponents.\n",
        "\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "def euclid_dist(p1, p2):\n",
        "    \"\"\"\n",
        "    Calculate the Euclidian Distance between two points\n",
        "    \n",
        "    Positional Arguments:\n",
        "        p1 -- A tuple of n numbers\n",
        "        p2 -- A tuple of n numbers\n",
        "    \n",
        "    Example:\n",
        "        p1 = (5,5)\n",
        "        p2 = (0,0)\n",
        "        p3 = (5,6,7,8,9,10)\n",
        "        p4 = (1,2,3,4,5,6)\n",
        "        print(euclid_dist(p1,p2)) #--> 7.0710678118654755\n",
        "        print(euclid_dist(p3,p4)) #--> 9.797958971132712\n",
        "    \"\"\"\n",
        "    n_sum=0\n",
        "    for i in range(len(p1)) :\n",
        "      n_sum += (p2[i]-p1[i])**2\n",
        "      \n",
        "    #-------------------- Reference --------------------\n",
        "    #dist = 0\n",
        "    #For all pairs of values in two points, Find difference and square\n",
        "    #for a, b in zip(p1,p2):\n",
        "    #  dist += (a-b)**2\n",
        "    #---------------------------------------------------\n",
        "\n",
        "    \n",
        "    # Take Square Root  \n",
        "    return float(n_sum**0.5)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 7",
          "locked": true,
          "points": "10",
          "solution": false
        },
        "id": "YgVB103TcShe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "7Ngj2bYAcShh",
        "colab_type": "text"
      },
      "source": [
        "#### Distances with `numpy`\n",
        "The above exercise is a simple check for understanding. However, in our eventual KNN function we will use `numpy` to more efficiently calculate distances with the following code : `np.linalg.norm(np.array(p1)-np.array(p2))`.  \n",
        "\n",
        "Thankfully because `Pandas` uses `Numpy` \"under the hood\" we will not have to cast to numpy arrays with `np.arry()` instead it will look like:  \n",
        "`np.linalg.norm(row1 - row1)`  \n",
        "\n",
        "Now that we can easily calculate the distances between any two points, we can start to build our function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "Yj3hNSsNcShi",
        "colab_type": "text"
      },
      "source": [
        "#### Question 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltgZsoIfcShj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### Code a function called \"all_distances\"\n",
        "### ACCEPT two inputs:\n",
        "### An observation from a data set.  e.g: har_train.iloc[50,:]\n",
        "### The full data set. e.g. har_train.\n",
        "\n",
        "### Create a <list> or numpy array of distances between:\n",
        "### ### that single point, and all points in the full dataset\n",
        "\n",
        "### RETURN the list of distances SORTED from smallest to largest.\n",
        "\n",
        "### Notes:\n",
        "### Use `np.linalg.norm()`, as described in above cell.\n",
        "### The smallest distance should be 0.\n",
        "\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "def all_distances(test_point, data_set):\n",
        "    \"\"\"\n",
        "    Find and return a list of distances between the \"test_point\"\n",
        "    and all the points in \"data_set\", sorted from smallest to largest.\n",
        "    \n",
        "    Positional Arguments:\n",
        "        test_point -- a Pandas Series corresponding to a row in \"data_set\"\n",
        "        data_set -- a Pandas DataFrame\n",
        "    \n",
        "    Example:\n",
        "        test_point = har_train.iloc[50,:]\n",
        "        data_set = har_train\n",
        "        \n",
        "        print(all_distances(test_point, data_set)[:5])\n",
        "        #--> [0.0, 2.7970187358249854, 2.922792670143521, 2.966555149052483, 3.033982453218797]\n",
        "    \n",
        "    \"\"\"\n",
        "    dis_data = []\n",
        "    \n",
        "    for i in data_set.index.values:                 \n",
        "      dis_data.append(np.linalg.norm(data_set.iloc[i] - test_point))  \n",
        "    \n",
        "    dis_data.sort()\n",
        "    \n",
        "    #---------------- Reference ----------------\n",
        "    # Take difference\n",
        "    #diff = test_point - data_set\n",
        "    \n",
        "    # Find distance\n",
        "    #dists = np.apply_along_axis(np.linalg.norm, 1, diff )\n",
        "    \n",
        "    # Sort\n",
        "    #dists = np.sort(dists)\n",
        "    #-------------------------------------------\n",
        "\n",
        "    \n",
        "    \n",
        "    return dis_data\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 8",
          "locked": true,
          "points": "15",
          "solution": false
        },
        "id": "XCVVxgfCcShl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "hrYCsGffcShp",
        "colab_type": "text"
      },
      "source": [
        "#### Question 9 \n",
        "\n",
        "Returning the value of a point and the label associated with that point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "wbwb2VIEcShp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### Code a function called \"labels_of_smallest\"\n",
        "### ACCEPT three inputs:\n",
        "### 1&2: numpy arrays, corresponding to 1: a numeric column and 2: a label column.\n",
        "### ### The i-th member of the numeric column corresponds to the i-th member of the label column\n",
        "### 3: an integer (>0); n.\n",
        "\n",
        "### RETURN a list (or numpy array) of the n labels corresponding to \n",
        "### ### the n smallest values in the numeric column.\n",
        "### NOTE: Make sure the order of labels corresponds to the order of values.\n",
        "\n",
        "### Hint: The labels are found in har_train_labels or y\n",
        "### Hint: `pd.concat()` might be useful for this or subsequent exercisces  \n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "def labels_of_smallest(numeric, labels, n):\n",
        "    \n",
        "    \"\"\"\n",
        "    Return the n labels corresponding to the n smallest values in the \"numeric\"\n",
        "    numpy array.\n",
        "    \n",
        "    Positional Arguments:\n",
        "        numeric -- a numpy array of numbers\n",
        "        labels -- a numpy array of labels (string or numeric)\n",
        "            corresponding to the values in \"numeric\"\n",
        "        n -- a positive integer\n",
        "        \n",
        "    Example:\n",
        "        numeric = np.array([7,6,5,4,3,2,1])\n",
        "        labels = np.array([\"a\",\"a\",\"b\",\"b\",\"b\",\"a\",\"a\"])\n",
        "        n = 6\n",
        "        \n",
        "        print(labels_of_smallest(numeric, labels, n))\n",
        "        #--> np.array(['a', 'a', 'b', 'b', 'b', 'a'])\n",
        "    \"\"\"\n",
        "    sortlabel = []\n",
        "    \n",
        "    mrg_np = list(zip(numeric, labels))           # Make list as 2 dimentionaly [(7, 'a'), ...., (1, 'a')\n",
        "    mrg_np.sort()                                 # Sorting by numeric\n",
        "    for i in range(n):\n",
        "      sortlabel.append(mrg_np[i][1])\n",
        "    \n",
        "    #--------------------  Reference -----------------------\n",
        "    # Create a df of the two arrays (to simplify sorting)\n",
        "    #con = np.concatenate((numeric.reshape(-1,1), labels.reshape(-1,1)), axis = 1)\n",
        "    #df = pd.DataFrame(con, columns = [\"num\",\"lab\"])\n",
        "    \n",
        "    # Sort\n",
        "    #df = df.sort_values(by = 'num')\n",
        "    \n",
        "    # Return the top \"n\" values\n",
        "    #return df['lab'].head(n).values\n",
        "     #-------------------------------------------\n",
        "    \n",
        "    return np.array(sortlabel)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 9",
          "locked": true,
          "points": "15",
          "solution": false
        },
        "id": "RkT9ksbkcShs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "1XmiW6BfcShy",
        "colab_type": "text"
      },
      "source": [
        "#### Question 10: \n",
        "Voting.\n",
        "Hint: look at [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) and `.most_common()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrmac6bBcShz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "from collections import Counter\n",
        "### Build a function called \"label_voting\"\n",
        "### ACCEPT a non-empty numpy array of labels as input\n",
        "### RETURN the value that appears most frequently in that array\n",
        "### In the case of of a tie, RETURN the value in the tie that appears first in the array\n",
        "\n",
        "\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "def label_voting(labels):\n",
        "    \"\"\"\n",
        "    Given a numpy array of labels. Return the label that appears most frequently\n",
        "    If there is a tie for most frequent, return the label that appears first.\n",
        "    \n",
        "    Positional Argument:\n",
        "        labels -- a numpy array of labels\n",
        "    \n",
        "    Example:\n",
        "        lab1 = np.array([1,2,2,3,3])\n",
        "        lab2 = np.array([\"a\",\"a\",\"b\",\"b\",\"b\"])\n",
        "        \n",
        "        print(label_voting(lab1)) #--> 2\n",
        "        print(label_voting(lab2)) #--> \"b\"\n",
        "        \n",
        "    ___________________Reference from Assignment __________________\n",
        "    # List methods used in this function, recast labels as list\n",
        "    labels = list(labels)\n",
        "    \n",
        "    # instantiate counter, find most common, returns tuples\n",
        "    c = Counter(labels).most_common()\n",
        "    \n",
        "    # If only one value present, return that value\n",
        "    if len(c) == 1:\n",
        "      return c[0][0]\n",
        "      \n",
        "    # IF first has majority, return first\n",
        "    if c[0][1] > c[1][1]:\n",
        "      return c[0][0]\n",
        "    # Otherwise, check to see which comes first in list\n",
        "    else:\n",
        "      top_votes = c[0][1]\n",
        "      #print(top_votes)\n",
        "      poss = []\n",
        "      for t in c:\n",
        "        if t[1] == top_votes:\n",
        "          poss.append(t[0])\n",
        "      \n",
        "      idx = dict()\n",
        "      # print(poss)\n",
        "      for p in poss:\n",
        "        idx[labels.index(p)] = p\n",
        "      #print(idx)\n",
        "      return labels[sorted(idx.keys())[0]]\n",
        "      \n",
        "      \n",
        "##### ALTERNATE FUNCTION\n",
        "def alt(labels):\n",
        "          \n",
        "    # Empty Dictionary\n",
        "    count = {}\n",
        "      \n",
        "    # Save length of labels\n",
        "    total = len(labels)\n",
        "      \n",
        "    # Go through labels, add entry to dictionary for each with:\n",
        "    # count and first occurrence\n",
        "    for i, l in enumerate(labels):\n",
        "      if l in count:\n",
        "        count[l]['count'] +=1\n",
        "      else:\n",
        "        count[l] = {'count':1, 'first':total -i}\n",
        "    \n",
        "    # Create DataFrame from dict, sort, and return top result\n",
        "    df = pd.DataFrame.from_dict(count).T\n",
        "    df = df.sort_values(by = ['count','first'], ascending = False)\n",
        "    \n",
        "    # Ret\n",
        "    return df.index[0]\n",
        "        \n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "    count_list = Counter(labels)\n",
        "    most_occu = count_list.most_common()  #Return Vairable Name , Frequency times\n",
        "    \n",
        "    return most_occu[0][0]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 10",
          "locked": true,
          "points": "15",
          "solution": false
        },
        "id": "rQu6XBUscSh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "spONYQtbcSh4",
        "colab_type": "text"
      },
      "source": [
        "#### Question 11\n",
        "Time to put everything together.  \n",
        "\n",
        "Question 6/7/8 involved calculating distances.\n",
        "Question 9 involved sorting and returning n labels.\n",
        "Question 10 counted \"votes.\"\n",
        "\n",
        "The next question asks for a KNN modeling function:  \n",
        "\n",
        "Given four inputs: \n",
        "1. a single value from  X_test (created above in our `test_train_split`)  \n",
        "2. X_train  \n",
        "3. y_train (labels)   \n",
        "4. n - the number of nearest neighbors to poll in making predictions.\n",
        "\n",
        "Create a function that:\n",
        "1. Calculates the Euclidean distance between that X_test-point and every point in X_train\n",
        "2. Finds the labels from the \"n\" nearest neighbors (ordered from closest to furthest)\n",
        "3. Returns a prediction according to the voting rules outlined above (simple majority - tie goes to closest/first)  \n",
        "\n",
        "Assign to \"`custom_KNN`\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12H0qVzxcSh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### Follow directions above\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "def custom_KNN( point, X_train, y_train, n):\n",
        "    \"\"\"\n",
        "    Predict the label for a single point, given training data and a specified\n",
        "    \"n\" number of neighbors.\n",
        "    \n",
        "    Positional Arguments:\n",
        "        point -- a pandas Series corresponding to an observation of a point with\n",
        "             unknown label.\n",
        "        x_train -- a pandas DataFrame corresponding to the measurements\n",
        "            of points in a dataset. Assume all values are numeric, and\n",
        "            observations are in the rows; features in the columns\n",
        "        y_train -- a pandas Series corresponding to the labels for the observations\n",
        "            in x_train\n",
        "    \n",
        "    Example:\n",
        "        point = pd.Series([1,2])\n",
        "        X_train = pd.DataFrame([[1,2],[3,4],[5,6]])\n",
        "        y_train = pd.Series([\"a\",\"a\",\"b\"])\n",
        "        n = 2\n",
        "        print(custom_KNN(point, X_train, y_train, n)) #--> 'a'\n",
        "        \n",
        "    ________________ Reference _______________    \n",
        "    # Helper Function for vote counting    \n",
        "    def countVotes(l):\n",
        "        c = Counter(l).most_common()\n",
        "        \n",
        "        if len(c) == 1:\n",
        "          return c[0][0]\n",
        "          \n",
        "        if c[0][1] > c[1][1]:\n",
        "          return c[0][0]\n",
        "          \n",
        "        else:\n",
        "          top_votes = c[0][1]\n",
        "          #print(top_votes)\n",
        "          poss = []\n",
        "      \n",
        "          for t in c:\n",
        "            if t[1] == top_votes:\n",
        "              poss.append(t[0])\n",
        "      \n",
        "          idx = dict()\n",
        "          # print(poss)\n",
        "      \n",
        "          for p in poss:\n",
        "            idx[l.index(p)] = p\n",
        "        \n",
        "          #print(idx)\n",
        "          return l[sorted(idx.keys())[0]]\n",
        "      \n",
        "    # Take difference\n",
        "    diff = point - X_train\n",
        "\n",
        "    # Find distance\n",
        "    dists = np.apply_along_axis(np.linalg.norm, 1, diff )\n",
        "\n",
        "    # Create df of distances; re-index to original data\n",
        "    df = pd.DataFrame(dists)\n",
        "    df.index = X_train.index\n",
        "\n",
        "    # Add labels, column names.\n",
        "    df = pd.concat([df, y_train], axis = 1)\n",
        "    df.columns = [\"dist\",\"label\"]\n",
        "\n",
        "    # Take top votes, and count\n",
        "    votes = list(df.sort_values(\"dist\").head(n)['label'])\n",
        "    return countVotes(votes)\n",
        "    \n",
        "    \"\"\"\n",
        "    point_list = list(point)     # change Panda Series into list to compare value \n",
        "  \n",
        "    for i in X_train.index.values:\n",
        "        if list(X_train.iloc[i,:]) == point_list:\n",
        "          print(\"y value \",y_train[i])\n",
        "    \n",
        "    pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 11",
          "locked": true,
          "points": "5",
          "solution": false
        },
        "scrolled": false,
        "id": "MxAclsG0cSh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "N7Y3udXPcSh_",
        "colab_type": "text"
      },
      "source": [
        "You should now have a functioning KNN classifier assigned to the function `customKNN`.\n",
        "\n",
        "Let's now see how good our classifier is using `n` = 5.  \n",
        "\n",
        "Be warned, the below cell may or may not complete runing on Vocareum due to processing constraints.  (The cell took 12.9s on my machine, using my fairly efficient function, a less efficient function took ~5.5mins).  \n",
        "\n",
        "**FOR FASTER GRADING, TRY COMMENTING OUT THE CELL BELOW**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "aWUik7sJcSh_",
        "colab_type": "code",
        "outputId": "4a42d524-9cc8-4073-d523-a2e4097249a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1373
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Create New tts\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=24)\n",
        "\n",
        "print(\"Total 'test' observations:\", len(X_test))\n",
        "print(\"Classifying every point in X_test would take too long - classify the first 200\")\n",
        "custom_preds = []\n",
        "for i, idx in enumerate(X_test.index[:200]):\n",
        "    if i % 100 == 0: print(i)\n",
        "    pred = custom_KNN(X_test.loc[idx,:], X_train, y_train, 5)\n",
        "    custom_preds.append(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total 'test' observations: 2206\n",
            "Classifying every point in X_test would take too long - classify the first 200\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-acd2492f7b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n# Create New tts\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=24)\\n\\nprint(\"Total \\'test\\' observations:\", len(X_test))\\nprint(\"Classifying every point in X_test would take too long - classify the first 200\")\\ncustom_preds = []\\nfor i, idx in enumerate(X_test.index[:200]):\\n    if i % 100 == 0: print(i)\\n    pred = custom_KNN(X_test.loc[idx,:], X_train, y_train, 5)\\n    custom_preds.append(pred)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-6a2ede2854b5>\u001b[0m in \u001b[0;36mcustom_KNN\u001b[0;34m(point, X_train, y_train, n)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpoint_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y value \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1735\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1737\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1738\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Too many indexers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 raise ValueError(\"Location based indexing can only have \"\n\u001b[1;32m    206\u001b[0m                                  \u001b[0;34m\"[{types}] types\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_type\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1670\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_valid_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_is_valid_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1713\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siNwAk2eZVKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "6Y8mJmOZcSiD",
        "colab_type": "text"
      },
      "source": [
        "<a id = \"sklearn\"></a>\n",
        "### KNN in Sklearn\n",
        "\n",
        "While useful to see exactly how predictions are made using K-Nearest Neighbors, the `sklearn` has an implementation called [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) that will run much faster than our home-built version.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "0sgvtjB3cSiD",
        "colab_type": "code",
        "outputId": "091e5f58-8c8e-4d25-97fe-91d9a28d2a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1237
        }
      },
      "source": [
        "%%time\n",
        "# Import\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Instantiate classifier\n",
        "# NB: Default distance is Euclidean\n",
        "knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "\n",
        "# Fit model with training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Create predictions for first 200 test observations\n",
        "# # (As was done above with customKNN)\n",
        "skpreds = knn.predict(X_test[:200])\n",
        "\n",
        "print(\"sklearn prediction performance\")\n",
        "print(classification_report(y_test[:200], skpreds))\n",
        "\n",
        "\n",
        "### The below lines of code will compare the performance of your home-built classification with\n",
        "### The sklearn predictions -- if all the cells above were run sucessfully, you should see identical scores\n",
        "\n",
        "print(\"\\nHome-Built prediction performance\")\n",
        "print(classification_report(y_test[:200], custom_preds))\n",
        "\n",
        "\n",
        "### The below lines of code will explicitly compare predictions:\n",
        "### \"differences\" should == 0!\n",
        "\n",
        "### NB: Commenting/uncommenting multiple lines in Jupyter can be accomplished with:\n",
        "### <ctrl-/> on windows and <cmd-/> on mac\n",
        "differences = 0\n",
        "for cust, sk in zip(custom_preds, skpreds):\n",
        "    if cust != sk:\n",
        "        differences +=1\n",
        "print(\"Total Differences:\", differences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sklearn prediction performance\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        44\n",
            "           2       0.97      1.00      0.99        33\n",
            "           3       1.00      0.95      0.98        22\n",
            "           4       0.82      0.92      0.87        25\n",
            "           5       0.94      0.86      0.90        36\n",
            "           6       1.00      1.00      1.00        40\n",
            "\n",
            "   micro avg       0.96      0.96      0.96       200\n",
            "   macro avg       0.96      0.96      0.95       200\n",
            "weighted avg       0.96      0.96      0.96       200\n",
            "\n",
            "\n",
            "Home-Built prediction performance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6c558582122d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'# Import\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.metrics import classification_report\\n\\n# Instantiate classifier\\n# NB: Default distance is Euclidean\\nknn = KNeighborsClassifier(n_neighbors = 5)\\n\\n# Fit model with training data\\nknn.fit(X_train, y_train)\\n\\n# Create predictions for first 200 test observations\\n# # (As was done above with customKNN)\\nskpreds = knn.predict(X_test[:200])\\n\\nprint(\"sklearn prediction performance\")\\nprint(classification_report(y_test[:200], skpreds))\\n\\n\\n### The below lines of code will compare the performance of your home-built classification with\\n### The sklearn predictions -- if all the cells above were run sucessfully, you should see identical scores\\n\\nprint(\"\\\\nHome-Built prediction performance\")\\nprint(classification_report(y_test[:200], custom_preds))\\n\\n\\n### The below lines of code will explicitly compare predictions:\\n### \"differences\" should == 0!\\n\\n### NB: Commenting/uncommenting multiple lines in Jupyter can be accomplished with:\\n### <ctrl-/> on windows and <cmd-/> on mac\\ndifferences = 0\\nfor cust, sk in zip(custom_preds, skpreds):\\n    if cust != sk:\\n        differences +=1\\nprint(\"Total Differences:\", differences)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[1;32m   1522\u001b[0m     \"\"\"\n\u001b[1;32m   1523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1524\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [200, 0]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "-nKlcsQQcSiF",
        "colab_type": "text"
      },
      "source": [
        "#### Practice with `sklearn`:  \n",
        "\n",
        "The below questions will ask you to create a new test/train split, and fit a new KNN model with `sklearn`.  \n",
        "\n",
        "All of these steps have already been performed above. Feel free to reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "MYqvASCMcSiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ensure Data is consistent\n",
        "\n",
        "# read feature names\n",
        "feats = pd.read_table(FEATURE_NAMES, sep='\\n', header=None)\n",
        "\n",
        "# read in training data\n",
        "har_train = pd.read_table(TRAIN_DATA, sep='\\s+', header=None)\n",
        "\n",
        "# read in training labels, and clean them.\n",
        "har_train_labels = pd.read_table(TRAIN_LABELS, sep='\\n', header=None)\n",
        "clean_features = [feat[0].split(' ')[1] for feat in feats.values]\n",
        "har_train.columns = clean_features\n",
        "\n",
        "har_train_labels = pd.read_table(TRAIN_LABELS, sep='\\n', header=None)\n",
        "har_train_labels.columns = ['label']\n",
        "y = har_train_labels.loc[:, 'label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "6g6AyLzqcSiI",
        "colab_type": "text"
      },
      "source": [
        "#### Question 12\n",
        "Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtvhgDBGcSiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### Making a new test-train-split on our data:\n",
        "### ### labels found in \"y\" and observations in `har_train`\n",
        "### ### For split, specify - test_size of .4 and a random_state of 1738.\n",
        "### assign output from the split to X_train2, X_test2, y_train2, y_test2 -- take care, the \"X\"s are capitlaized\n",
        "### and the \"y\"s are lower-case.\n",
        "### Which of the following would accomplish that task?\n",
        "\n",
        "### 'a') X_train2, X_test2, y_train2, y_test2 = train_test_split(har_train, y, test_size = .4, random_state = 1738)\n",
        "### 'b') X_train2, X_test2, y_train2, y_test2 = train_test_split(har_train, y, train_size = .4, random_state = 1738)\n",
        "### 'c') X_train2, X_test2, y_train2, y_test2 = train_test_split(har_train, y, .4, 1738)\n",
        "### 'd') X_train2, X_test2, y_train2, y_test2 = train_test_split(har_train, y, t_size = .4, rs = 1738)\n",
        "\n",
        "### Assign character associated with you choice as string to ans1\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "ans1 = 'a'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 12",
          "locked": true,
          "points": "10",
          "solution": false
        },
        "id": "Yj2LtYFzcSiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "YG8tJJbCcSiO",
        "colab_type": "text"
      },
      "source": [
        "#### Question 13"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "jt2ug8WIcSiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### This cell creates X_train3, X_test3, y_train3, and y_test3; used below.\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(har_train, y, test_size = .4, random_state = 2001)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ3PNpNicSiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### Build a KNN classifier using sklearn.\n",
        "### ### specify n_neighbors as 10. Otherwise accept the `KNeighborsClassifier` defaults.\n",
        "\n",
        "### Fit the model using the provided \"X_train3\" and \"y_train3\" variables, from above cell.\n",
        "### assign the predictions from your model for the provided \"X_test3\" data to a variable called ans1.\n",
        "### YOUR ANSWER BELOW\n",
        "\"\"\"\n",
        "Example:\n",
        "\n",
        "# Code for Instantiating a KNN Model\n",
        "\n",
        "# Fit KNN with X_train3 and y_train3\n",
        "\n",
        "# Create Predictions on X_test3\n",
        "\n",
        "ans1 = preds\n",
        "\n",
        "print(ans1[:5])\n",
        "#-->np.array([6 5 1 6 4])\n",
        "\n",
        "### NOTE: Your predictions may look different due to how \"random_state\" depends on current time.\n",
        "\"\"\"\n",
        "clas = KNeighborsClassifier(n_neighbors=10)\n",
        "clas.fit(X_train3, y_train3)\n",
        "preds = clas.predict(X_test3)\n",
        "ans1 = preds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 13",
          "locked": true,
          "points": "10",
          "solution": false
        },
        "id": "M3FwFoTIcSiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "MZ25paeIcSiY",
        "colab_type": "text"
      },
      "source": [
        "Building a model using sklearn is just as easy as those last two steps! As long as your data is in the right format, once you make your train/test split, the syntax for fitting pretty much any of the models in `sklearn` is about the same.   \n",
        "\n",
        "<a id=\"part3\"></a>\n",
        "## Part 3: Interpret Results\n",
        "\n",
        "For interpreting results we will be looking at the tradeoff between bias and variance as we change our `n_neighbors`. In many cases, false negatives are more costly and false positives. As such we will be looking primarily at the change in recall as we build a number of different models.  \n",
        "\n",
        "Note: The below takes some time to run. ~ 3.5min. Thus output is provided as screenshot below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "FSMjNrgPcSia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# from sklearn.metrics import recall_score\n",
        "\n",
        "### Calculating Recal scores for multiple \"n-neighbors\"\n",
        "# recall_scores = {}\n",
        "# for n in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,20,25,50,75,100]:\n",
        "#     knn = KNeighborsClassifier(n_neighbors=n)\n",
        "#     knn.fit(X_train, y_train)\n",
        "#     recall_scores[n] = recall_score(y_test, knn.predict(X_test), average = None)\n",
        "    \n",
        "### Put recall scores into DataFrame\n",
        "# scores_df = pd.DataFrame(recall_scores).T\n",
        "# scores_df.columns = [str(i) for i in range(1,7)]\n",
        "# scores_df.index = scores_df.index.astype(str)\n",
        "\n",
        "### Create plot of recall scores\n",
        "# plt.figure(figsize = (10,10))\n",
        "# for col in scores_df:\n",
        "#     if col != 'n_neighbors':\n",
        "#         plt.plot(scores_df[col], label = col)\n",
        "    \n",
        "# plt.ylabel(\" Recall Score\", fontsize = 12)\n",
        "# plt.xlabel(\"n_neighbors (NB: not an interval scale)\", fontsize = 12)\n",
        "# plt.legend(title = \"activity\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "zjn4MKqLcSic",
        "colab_type": "text"
      },
      "source": [
        "#### Output\n",
        "![recall](./assets/recall.PNG)  \n",
        "\n",
        "#### Question 14"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt8yaeEKcSid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### Looking at the recall scores above;\n",
        "### as n_neighbors trends towards 100 do we see in increase in:\n",
        "\n",
        "### 'a') bias\n",
        "### 'b') variance\n",
        "\n",
        "### Assign the string associated with your choice to ans1\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "ans1 = 'a'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 14",
          "locked": true,
          "points": "5",
          "solution": false
        },
        "id": "5bbg4wZYcSif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "BWFoRQsbcSig",
        "colab_type": "text"
      },
      "source": [
        "#### Question 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        },
        "id": "MtrfBTwKcSih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### In looking at the recall scores above, does it look like the bestter KNN models have:\n",
        "\n",
        "### 'a') n_neighbors >= 15\n",
        "### 'b') n_neigbors < 15\n",
        "\n",
        "### Assign the string associated with your shoice to ans1\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "ans1 = 'b'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 15",
          "locked": true,
          "points": "5",
          "solution": false
        },
        "id": "q9l5Q29_cSim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "gcjJtOmIcSip",
        "colab_type": "text"
      },
      "source": [
        "#### Question 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PKPuoa1cSis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED\n",
        "### What might explain the ups-and-downs of recall in activities 4 and 6?\n",
        "\n",
        "### 'a') calculation of Euclidean Distance\n",
        "### 'b') Use of Entropy (information gain) for splitting\n",
        "### 'c') tie-breaking/voting proceedures\n",
        "### 'd') Simply a feature of KNN models: unavoidable\n",
        "### Assign the string associated with your choice to ans1\n",
        "\n",
        "### YOUR ANSWER BELOW\n",
        "\n",
        "ans1 = 'c'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "Question 16",
          "locked": true,
          "points": "5",
          "solution": false
        },
        "id": "hLlZKNjbcSiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# AUTOGRADER TEST - DO NOT REMOVE\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "yTAC-1OzcSix",
        "colab_type": "text"
      },
      "source": [
        "For this investigation of our model, recall was used. In other instances other metrics might be more useful, or more translatable to your use-case.  \n",
        "\n",
        "Hopefully from this brief look into tuning our model and investigating the effects it has become clear that either \"success\" or \"failure\" might be due to circumstance.  \n",
        "\n",
        "Even in the search for the trade-off between bias and variance, a single observation or even a pair of observations might not tell the full story."
      ]
    }
  ]
}